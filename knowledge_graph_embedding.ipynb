{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "knowledge_graph_embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsstnaka/machine_learning_handson/blob/master/knowledge_graph_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8El-LUsQpgU"
      },
      "source": [
        "!wget -nc https://github.com/nju-websoft/JAPE/raw/master/data/dbp15k.tar.gz\n",
        "!tar zxf dbp15k.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjjTUzkWRKyx"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7A0BO4cLqQa"
      },
      "source": [
        "# settings\n",
        "EMBEDDING_DIM = 50\n",
        "TRAIN_BATCH_SIZE = 256\n",
        "TEST_BATCH_SIZE = 128\n",
        "SCORE_FUNC = 'L1'  # 'L1' or 'L2'\n",
        "NORMALIZE = True  # set True to normalize embedding\n",
        "EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLNPINcrSSLE"
      },
      "source": [
        "df = pd.read_csv('dbp15k/ja_en/s_triples', sep='\\t', names=['head', 'relation', 'tail'])\n",
        "print(len(df))\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_NY2KvwUHLt"
      },
      "source": [
        "df = df.applymap(lambda x: x.split('/')[-1])\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZIGb9VKWdIO"
      },
      "source": [
        "entity_set = set(df['head']) | set(df['tail'])\n",
        "relation_set = set(df['relation'])\n",
        "len(entity_set), len(relation_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibB8xm4VXZ5M"
      },
      "source": [
        "entity_idx_dic = {name: idx for idx, name in enumerate(entity_set)}\n",
        "relation_idx_dic = {name: idx for idx, name in enumerate(relation_set)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXEQGnMIIEq8"
      },
      "source": [
        "df['head_idx'] = df['head'].apply(lambda x: entity_idx_dic[x])\n",
        "df['relation_idx'] = df['relation'].apply(lambda x: relation_idx_dic[x])\n",
        "df['tail_idx'] = df['tail'].apply(lambda x: entity_idx_dic[x])\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LxIpjXyKeEN"
      },
      "source": [
        "df.drop(['head', 'relation', 'tail'], axis=1, inplace=True)\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7jPZ33eIs15"
      },
      "source": [
        "head_count = df.groupby('relation_idx')['head_idx'].apply(set).apply(len)\n",
        "tail_count = df.groupby('relation_idx')['tail_idx'].apply(set).apply(len)\n",
        "tail_prob = tail_count / (head_count + tail_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLAQxc0p9gIL"
      },
      "source": [
        "def data_generator(num_entities, triples, prob):\n",
        "    triple_set = {(row[0], row[1], row[2]) for row in triples}\n",
        "    for head, relation, tail in np.random.permutation(triples):\n",
        "        neg_head = None\n",
        "        neg_tail = None\n",
        "        if random.random() > prob[relation]:\n",
        "            # replace head\n",
        "            neg_tail = tail\n",
        "            while True:\n",
        "                neg_head = random.randrange(num_entities)\n",
        "                if (neg_head, relation, tail) not in triple_set:\n",
        "                    break\n",
        "        else:\n",
        "            # replace tail\n",
        "            neg_head = head\n",
        "            while True:\n",
        "                neg_tail = random.randrange(num_entities)\n",
        "                if (head, relation, neg_tail) not in triple_set:\n",
        "                    break\n",
        "        yield [head, relation, tail, neg_head, neg_tail]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bThmV3RALM45"
      },
      "source": [
        "train_ds = tf.data.Dataset.from_generator(data_generator, args=[len(entity_idx_dic), df.values, tail_prob], output_types=(tf.int64), output_shapes=(5,)).batch(TRAIN_BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jru8Ld79M9ij"
      },
      "source": [
        "def score_func(heads, tails, relations):\n",
        "    #return tf.square(tf.norm(heads + relations - tails, ord=2, axis=-1))\n",
        "    if SCORE_FUNC == 'L1':\n",
        "        return tf.norm(heads + relations - tails, ord=1, axis=-1)\n",
        "    elif SCORE_FUNC == 'L2':\n",
        "        return tf.square(tf.norm(heads + relations - tails, ord=2, axis=-1))\n",
        "    else:\n",
        "        raise Exception('Invalid SCORE_FUNC:', SCORE_FUNC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoQmvoZ_PYxZ"
      },
      "source": [
        "bound = 6 / math.sqrt(EMBEDDING_DIM)\n",
        "initializer = tf.keras.initializers.RandomUniform(minval=-bound, maxval=bound)\n",
        "entity_embeddings = tf.keras.layers.Embedding(len(entity_idx_dic), EMBEDDING_DIM,\n",
        "                                              embeddings_initializer=initializer)\n",
        "relation_embeddings = tf.keras.layers.Embedding(len(entity_idx_dic), EMBEDDING_DIM,\n",
        "                                                embeddings_initializer=initializer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f33qHK_dPezE"
      },
      "source": [
        "margin = 1.0\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "#optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "@tf.function\n",
        "def train_step(inputs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        heads = entity_embeddings(inputs[:, 0])\n",
        "        relations = relation_embeddings(inputs[:, 1])\n",
        "        tails = entity_embeddings(inputs[:, 2])\n",
        "        neg_heads = entity_embeddings(inputs[:, 3])\n",
        "        neg_tails = entity_embeddings(inputs[:, 4])\n",
        "        if NORMALIZE:\n",
        "            heads = tf.nn.l2_normalize(heads, axis=-1)\n",
        "            tails = tf.nn.l2_normalize(tails, axis=-1)\n",
        "            #relations = tf.nn.l2_normalize(relations, axis=-1)\n",
        "            neg_heads = tf.nn.l2_normalize(neg_heads, axis=-1)\n",
        "            neg_tails = tf.nn.l2_normalize(neg_tails, axis=-1)\n",
        "        pos_scores = score_func(heads, tails, relations)\n",
        "        neg_scores = score_func(neg_heads, neg_tails, relations)\n",
        "        loss = tf.reduce_sum(tf.maximum(pos_scores + margin - neg_scores, 0.0))\n",
        "    #print(variables)\n",
        "    variables = entity_embeddings.trainable_variables + relation_embeddings.trainable_variables\n",
        "    grads = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(grads, variables))\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gp_4iEjPqbT"
      },
      "source": [
        "for e in range(1, EPOCHS+1):\n",
        "    total_loss = 0.0\n",
        "    for batch_data in train_ds:\n",
        "        loss = train_step(batch_data)\n",
        "        total_loss += loss.numpy()\n",
        "    print(\"Epoch {}: loss={:.6f}\".format(e, total_loss))\n",
        "    #evaluate(valid_ds)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}